# CLAUDE.md - AI Assistant Entry Point

**READ THIS FIRST** - Every session starts here

---

## âš ï¸ IMPORTANT: Adaptable Documentation

**This file uses REFERENCES, not hard-coded structures.**

- âŒ **DON'T** rely on cached file trees or progress percentages
- âœ… **DO** use paths and check actual files for current state
- âœ… **DO** use `ls`, `find`, `pytest --collect-only` to see structure
- âœ… **DO** read `docs/current/PROJECT_STATUS.md` for latest progress
- âœ… **DO** check `test_reports/latest.txt` for most recent test results

**Reason**: Hard-coded structures go stale immediately. Always check the actual files.

---

## ğŸ¯ Purpose

This file tells Claude AI exactly what to read and in what order to get up to speed on this project.

**Last Updated**: 2025-10-28
**Project**: Risk Manager V34
**Location**: `C:\Users\jakers\Desktop\risk-manager-v34\`
**Environment**: Windows WSL2

---

## ğŸ“– Quick Start (First 5 Minutes)

### 1. Read These Files IN ORDER:

```
Priority 1 - Latest Session (1 min):
  1. AI_SESSION_HANDOFF_2025-10-28.md     - â­ MOST RECENT SESSION (100% rule tests!)
  2. docs/current/PROJECT_STATUS.md       - Current state & progress
  3. test_reports/latest.txt              - Most recent test results

Priority 2 - Testing System (3 min):
  4. docs/testing/README.md               - Testing navigation
  5. docs/testing/TESTING_GUIDE.md        - Core testing reference
  6. docs/testing/RUNTIME_DEBUGGING.md    - Runtime validation system

Priority 3 - SDK Integration (3 min):
  7. docs/current/SDK_INTEGRATION_GUIDE.md  - How we use Project-X SDK
  8. docs/current/MULTI_SYMBOL_SUPPORT.md   - Account-wide risk

Priority 4 - API Reference (For Coding/Testing):
  9. SDK_API_REFERENCE.md                   - Actual SDK & our API contracts
  10. SDK_ENFORCEMENT_FLOW.md               - Complete enforcement wiring
  11. TEST_RUNNER_FINAL_FIXES.md            - Test runner behavior & fixes
```

**After reading these 11 files, you will know**:
- âœ… What's been built and what's working
- âœ… Current test status (passing/failing)
- âœ… How testing and runtime debugging work
- âœ… How the SDK is integrated
- âœ… **Actual API contracts** (prevent test/code mismatches)
- âœ… **Enforcement flow** (how violations trigger SDK actions)
- âœ… **Test runner behavior** (why tests were failing/fixed)
- âœ… What we still need to build
- âœ… Exactly where we left off

---

## ğŸš€ Entry Points (How to Run the System)

### Development Runtime (Primary Entry Point)

**Command**: `python run_dev.py`

**Purpose**: Live microscope for validating the complete system end-to-end

**Use When**:
- First-time integration testing
- Validating rule math and event flow
- Debugging SDK integration
- Watching real-time events
- Before deploying as Windows Service

**Features**:
- âœ… Maximum logging (8 checkpoints visible)
- âœ… Real-time event streaming
- âœ… Rule evaluation display
- âœ… P&L tracking
- âœ… Enforcement action visibility
- âœ… Graceful Ctrl+C shutdown

**Options**:
```bash
python run_dev.py                    # Interactive account selection
python run_dev.py --account ACC123   # Explicit account
python run_dev.py --config path.yaml # Custom config
python run_dev.py --log-level DEBUG  # More verbose
```

**Documentation**: See `RUN_DEV_IMPLEMENTATION.md`

---

### Admin CLI (Management Interface)

**Command**: `python admin_cli.py`

**Purpose**: Configure, manage, and control the Risk Manager system

**Modes**:
1. **Interactive Menu** (default - no args)
   - Number-based navigation (1-6)
   - Setup wizard, service control, rule configuration
   - Dashboard, connection testing

2. **Command Mode** (with args)
   - Individual commands for scripting
   - `admin_cli service status`
   - `admin_cli rules list`
   - `admin_cli config show`

**Key Features**:
- âœ… Setup wizard (4-step, SDK-free)
- âœ… Service control (start/stop/restart)
- âœ… Rule management (13 rules)
- âœ… Configuration editing
- âœ… UAC elevation checks

**Documentation**: See `ADMIN_CLI_MVP_COMPLETE.md` and `ADMIN_MENU_IMPLEMENTATION.md`

---

### Test Runner

**Command**: `python run_tests.py`

**Purpose**: Interactive test menu for running pytest suites

**Use When**:
- Running unit/integration/E2E tests
- Checking test coverage
- Running runtime smoke tests
- Debugging test failures

**Documentation**: See `docs/testing/TESTING_GUIDE.md`

---

## ğŸ§ª Testing System Overview

### Interactive Test Runner Menu

**Command**: `python run_tests.py`

**Location**: Root directory (`run_tests.py`)

#### Menu Structure:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Risk Manager V34 - Test Runner                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Test Selection:
  [1] Run ALL tests
  [2] Run UNIT tests only
  [3] Run INTEGRATION tests only
  [4] Run E2E tests only
  [5] Run SLOW tests only
  [6] Run tests with COVERAGE report
  [7] Run tests with COVERAGE + HTML report
  [8] Run specific test file
  [9] Run tests matching keyword
  [0] Run last failed tests only

Runtime Checks:
  [s] Runtime SMOKE (DRY-RUN, fail-fast, 8s timeout)
  [r] Runtime SOAK (30-60s DRY-RUN)
  [t] Runtime TRACE (ASYNC_DEBUG=1, deep debug)
  [l] View/Tail LOGS
  [e] Env/Config SNAPSHOT
  [g] GATE: Tests + Smoke combo

Utilities:
  [v] Run in VERBOSE mode (shows each test)
  [c] Check COVERAGE status
  [p] View last test REPORT
  [h] Help - Testing with AI
  [q] Quit
```

### Test Reports Auto-Save

**Every test run automatically saves to**:
- `test_reports/latest.txt` - Always the most recent run (overwritten)
- `test_reports/YYYY-MM-DD_HH-MM-SS_passed.txt` - Timestamped pass
- `test_reports/YYYY-MM-DD_HH-MM-SS_failed.txt` - Timestamped fail

**Usage Pattern**:
```bash
# User runs tests
python run_tests.py
# â†’ Select option â†’ Report auto-saves

# AI reads latest results
Read test_reports/latest.txt

# The report contains:
# - Full pytest output with colors
# - Pass/fail status
# - Exit code
# - Timestamp
# - Complete tracebacks for failures
```

### Runtime Reliability Pack

**NEW: Prevents "Tests Green But Runtime Broken" Problem**

The Runtime Reliability Pack provides 5 capabilities:

#### 1. **Smoke Test** (`[s]` in menu)
- **What**: Boots system and validates first event fires within 8 seconds
- **Exit Codes**:
  - `0` = Success (first event observed)
  - `1` = Exception occurred (see logs)
  - `2` = Boot stalled (no events within timeout)
- **When**: After test suite passes, before deployment

#### 2. **Soak Test** (`[r]` in menu)
- **What**: Extended 30-60s runtime validation
- **Why**: Catches memory leaks, deadlocks, resource exhaustion
- **When**: Before major deployments

#### 3. **Trace Mode** (`[t]` in menu)
- **What**: Deep async task debugging (ASYNC_DEBUG=1)
- **Output**: `runtime_trace.log` with all pending tasks
- **When**: Service starts but hangs/stalls

#### 4. **Log Viewer** (`[l]` in menu)
- **What**: Stream logs in real-time or view last 100 lines
- **Location**: `data/logs/risk_manager.log`
- **When**: Debugging runtime issues

#### 5. **Env Snapshot** (`[e]` in menu)
- **What**: Shows configuration, env vars, Python version
- **When**: Configuration troubleshooting

**Implementation**:
- **Source**: `src/runtime/` (1,316 lines across 6 files)
- **Tests**: `tests/runtime/` (70 tests across 5 files)
- **Docs**: `docs/testing/RUNTIME_DEBUGGING.md` (36KB comprehensive guide)

---

## ğŸ” 8-Checkpoint Logging System

**Location**: SDK logging integrated in:
- `src/risk_manager/core/manager.py` - Checkpoints 1-4
- `src/risk_manager/core/engine.py` - Checkpoints 5-7
- `src/risk_manager/sdk/enforcement.py` - Checkpoint 8

**The 8 Strategic Checkpoints**:

```
Checkpoint 1: ğŸš€ Service Start
  â””â”€ Log: "Risk Manager starting..."
  â””â”€ Where: manager.py:start()

Checkpoint 2: âœ… Config Loaded
  â””â”€ Log: "Config loaded: X rules"
  â””â”€ Where: manager.py:_load_config()

Checkpoint 3: âœ… SDK Connected
  â””â”€ Log: "SDK connected: account_id"
  â””â”€ Where: manager.py:_connect_sdk()

Checkpoint 4: âœ… Rules Initialized
  â””â”€ Log: "Rules initialized: X rules"
  â””â”€ Where: manager.py:_initialize_rules()

Checkpoint 5: âœ… Event Loop Running
  â””â”€ Log: "Event loop running"
  â””â”€ Where: engine.py:start()

Checkpoint 6: ğŸ“¨ Event Received
  â””â”€ Log: "Event received: {event}"
  â””â”€ Where: engine.py:handle_event()

Checkpoint 7: ğŸ” Rule Evaluated
  â””â”€ Log: "Rule evaluated: {rule} {result}"
  â””â”€ Where: engine.py:handle_event()

Checkpoint 8: âš ï¸ Enforcement Triggered
  â””â”€ Log: "Enforcement triggered: {action}"
  â””â”€ Where: enforcement.py:enforce()
```

**Reading Logs**:
```bash
# View logs via menu
python run_tests.py â†’ [l]

# Or directly
cat data/logs/risk_manager.log | grep "ğŸš€\|âœ…\|ğŸ“¨\|ğŸ”\|âš ï¸"

# Find where it stopped
cat data/logs/risk_manager.log | tail -20
```

---

## ğŸ“ Project Structure

**Critical Directories** (use paths, don't cache structure):

### Source Code
```
src/
â”œâ”€â”€ risk_manager/         # Core risk management system
â”‚   â”œâ”€â”€ core/            # Manager, engine, events (with SDK logging)
â”‚   â”œâ”€â”€ rules/           # Risk rule implementations
â”‚   â”œâ”€â”€ sdk/             # SDK integration layer (with enforcement logging)
â”‚   â”œâ”€â”€ state/           # State management
â”‚   â”œâ”€â”€ ai/              # AI integration
â”‚   â”œâ”€â”€ integrations/    # External integrations
â”‚   â””â”€â”€ monitoring/      # Monitoring & metrics
â””â”€â”€ runtime/             # Runtime reliability capabilities â­ NEW
    â”œâ”€â”€ smoke_test.py    # Boot validation (exit codes 0/1/2)
    â”œâ”€â”€ heartbeat.py     # Liveness monitoring (1s intervals)
    â”œâ”€â”€ async_debug.py   # Task dump for deadlock detection
    â”œâ”€â”€ post_conditions.py # System wiring validation
    â””â”€â”€ dry_run.py       # Deterministic mock event generator
```

### Tests
```
tests/
â”œâ”€â”€ unit/                # Unit tests (fast, mocked)
â”œâ”€â”€ integration/         # Integration tests (real SDK)
â”œâ”€â”€ runtime/             # Runtime reliability tests â­ NEW
â”‚   â”œâ”€â”€ test_smoke.py    # 13 smoke test scenarios
â”‚   â”œâ”€â”€ test_heartbeat.py
â”‚   â”œâ”€â”€ test_async_debug.py
â”‚   â”œâ”€â”€ test_post_conditions.py
â”‚   â””â”€â”€ test_dry_run.py
â”œâ”€â”€ fixtures/            # Shared test fixtures
â””â”€â”€ conftest.py          # Pytest configuration
```

### Documentation
```
docs/
â”œâ”€â”€ current/             # Active documentation
â”‚   â”œâ”€â”€ PROJECT_STATUS.md           # â­ START HERE - Current state
â”‚   â”œâ”€â”€ SDK_INTEGRATION_GUIDE.md    # How we use SDK
â”‚   â”œâ”€â”€ MULTI_SYMBOL_SUPPORT.md     # Account-wide risk
â”‚   â”œâ”€â”€ RULE_CATEGORIES.md          # Rule types
â”‚   â”œâ”€â”€ CONFIG_FORMATS.md           # YAML examples
â”‚   â””â”€â”€ SECURITY_MODEL.md           # Windows UAC security
â”œâ”€â”€ testing/             # Testing documentation â­ NEW
â”‚   â”œâ”€â”€ README.md                   # Testing navigation
â”‚   â”œâ”€â”€ TESTING_GUIDE.md            # Core testing reference
â”‚   â”œâ”€â”€ RUNTIME_DEBUGGING.md        # Runtime reliability guide
â”‚   â””â”€â”€ WORKING_WITH_AI.md          # AI workflow
â”œâ”€â”€ dev-guides/          # Developer guides
â”œâ”€â”€ PROJECT_DOCS/        # Original specifications (pre-SDK)
â””â”€â”€ archive/             # Archived old versions
```

### Test Reports
```
test_reports/            # Auto-generated test reports â­ NEW
â”œâ”€â”€ README.md            # Report format documentation
â”œâ”€â”€ latest.txt           # â­ Most recent test run (ALWAYS CHECK THIS)
â””â”€â”€ YYYY-MM-DD_HH-MM-SS_*.txt  # Timestamped archives
```

### Agent Guidelines
```
.claude/
â”œâ”€â”€ agents/              # Custom agent definitions
â”œâ”€â”€ commands/            # Custom slash commands
â””â”€â”€ prompts/             # Agent protocols
    â””â”€â”€ runtime-guidelines.md  # â­ Runtime testing protocols
```

### Configuration
```
config/
â”œâ”€â”€ risk_config.yaml     # Risk rules configuration
â”œâ”€â”€ accounts.yaml        # Account mappings
â””â”€â”€ *.yaml.template      # Configuration templates
```

---

## ğŸ“š Essential Documentation Paths

### Must Read First
- `docs/current/PROJECT_STATUS.md` - **START HERE** - Current progress
- `test_reports/latest.txt` - **CHECK THIS** - Most recent test results

### Testing & Debugging
- `docs/testing/TESTING_GUIDE.md` - Core testing reference
- `docs/testing/RUNTIME_DEBUGGING.md` - Runtime reliability guide â­ NEW
- `docs/testing/WORKING_WITH_AI.md` - AI workflow including runtime debugging
- `test_reports/README.md` - Test report format documentation
- `.claude/prompts/runtime-guidelines.md` - Agent runtime testing protocols

### SDK Integration
- `docs/current/SDK_INTEGRATION_GUIDE.md` - How to use Project-X SDK
- `docs/current/RULES_TO_SDK_MAPPING.md` - What SDK provides vs what we build

### Architecture
- `docs/current/MULTI_SYMBOL_SUPPORT.md` - Account-wide risk across symbols
- `docs/current/RULE_CATEGORIES.md` - Rule types (CRITICAL!)
- `docs/current/SECURITY_MODEL.md` - Windows UAC security

### Configuration
- `docs/current/CONFIG_FORMATS.md` - Complete YAML config examples

---

## ğŸ”‘ Key Understanding #1: SDK-First Approach

### CRITICAL CONTEXT

**The specs in `docs/PROJECT_DOCS/` were written BEFORE the Project-X SDK existed.**

At that time:
- We had to map everything to raw ProjectX Gateway API
- Manual WebSocket handling
- Manual state tracking
- Manual order/position management

**NOW we have the Project-X-Py SDK** which handles:
- âœ… WebSocket connections (SignalR)
- âœ… Real-time events (positions, orders, trades)
- âœ… Order management (place, cancel, modify)
- âœ… Position management (close, partially close)
- âœ… Account data & statistics
- âœ… Market data & indicators
- âœ… Auto-reconnection

### What This Means

**DON'T**: Reimplement what the SDK already does
**DO**: Use the SDK as foundation, add risk management on top

**Example**:
- âŒ OLD: Write custom SignalR listener from specs
- âœ… NEW: Use `TradingSuite` from SDK, wrap with risk logic

See `docs/current/SDK_INTEGRATION_GUIDE.md` for mapping.

---

## ğŸ” Key Understanding #2: Windows UAC Security

### CRITICAL: NO CUSTOM PASSWORDS

**The system uses Windows UAC (User Account Control) for admin protection.**

**NO custom passwords stored. NO authentication database. Just Windows admin rights.**

### Why This Matters

The Risk Manager is designed to be **"virtually unkillable"** by the trader:

- âœ… Windows Service runs as LocalSystem (highest privilege)
- âœ… Trader CANNOT kill the service without Windows admin password
- âœ… Trader CANNOT edit configs (protected by Windows ACL)
- âœ… Trader CANNOT stop the service via Task Manager
- âœ… Admin CLI requires Windows elevation (UAC prompt)
- âœ… Only Windows admin password can override protection

**This is NOT a custom password system. This is OS-level security.**

### Two Access Levels

```
Admin CLI:  Right-click terminal â†’ "Run as Administrator"
            â†’ Windows UAC prompt â†’ Enter Windows admin password
            â†’ Full control (configure, unlock, stop service)

Trader CLI: Normal terminal (no elevation needed)
            â†’ View-only access
            â†’ See status, P&L, lockouts, logs
            â†’ CANNOT modify anything
```

**See `docs/current/SECURITY_MODEL.md` for complete details.**

---

## ğŸ§ª Key Understanding #3: Testing Hierarchy

### The Testing Pyramid

```
        /\
       /E2E\        10% - Full system, realistic scenarios
      /------\
     /  Integ \     30% - Real SDK, component interaction
    /----------\
   / Unit Tests \   60% - Fast, isolated, mocked
  /--------------\
```

### Runtime Validation Layer â­ NEW

**Beyond Pytest**: Runtime Reliability Pack prevents "tests green but runtime broken"

```
Step 1: pytest (automated)
  â””â”€ [2] Unit tests
  â””â”€ [3] Integration tests
  â””â”€ [4] E2E tests
  â””â”€ âœ… All pass

Step 2: Runtime validation (enforced liveness)
  â””â”€ [s] Smoke Test â† Proves first event fires within 8s
  â””â”€ Exit code 0 = Actually works!
  â””â”€ Exit code 1 = Exception (fix it)
  â””â”€ Exit code 2 = Boot stalled (check subscriptions)

Step 3: Deploy
  â””â”€ Confident it works!
```

**Why Both?**
- **pytest** validates logic correctness
- **Runtime Pack** validates system liveness
- **Together** prevent "tests pass but nothing happens"

---

## ğŸ”§ Common Tasks Reference

### âš ï¸ BEFORE Writing Code or Tests - Read API Contracts!

**CRITICAL**: Always check actual API signatures to prevent test/code mismatches!

```bash
# 1. Check our API contracts
Read SDK_API_REFERENCE.md

# 2. Check actual implementation
from risk_manager.core.events import RiskEvent
import inspect
print(inspect.signature(RiskEvent.__init__))

# 3. Verify enum values exist
from risk_manager.core.events import EventType
print([e.name for e in EventType])
```

**Common Mistakes to Avoid**:
- âŒ Using `RiskEvent(type=...)` â†’ âœ… Use `RiskEvent(event_type=...)`
- âŒ Using `EventType.TRADE_EXECUTED` â†’ âœ… Check if it exists first!
- âŒ Using `PnLTracker(account_id=...)` â†’ âœ… Use `PnLTracker(db=...)`

**See**: `SDK_API_REFERENCE.md` for all actual API contracts

---

### Run Tests (Interactive Menu)
```bash
# Interactive test runner with report generation
python run_tests.py

# Menu auto-saves reports to:
# - test_reports/latest.txt (most recent)
# - test_reports/YYYY-MM-DD_HH-MM-SS_passed.txt (timestamped)
```

**Key Menu Options**:
- `[2]` - Unit tests (fast, use this most often)
- `[3]` - Integration tests (requires SDK connection)
- `[s]` - Runtime smoke test (validates boot + first event)
- `[g]` - Gate test (unit + integration + smoke combo)
- `[p]` - View last test report

### When User Says "Fix Test Errors"
```bash
# 1. Read the latest test report
Read test_reports/latest.txt

# The report contains:
# - Full pytest output with colors
# - Failures with complete tracebacks
# - Warnings
# - Summary statistics
# - Exit code
```

**Workflow**:
1. User runs tests via menu â†’ Auto-saves to `test_reports/latest.txt`
2. User says "fix test errors"
3. AI reads `test_reports/latest.txt`
4. AI identifies failures and fixes them
5. Repeat until all tests pass

### When Tests Pass But Runtime Fails

**Symptom**: Tests green âœ… but service starts and nothing happens

**Solution**: Run Runtime Smoke Test
```bash
python run_tests.py
# Select [s] Runtime SMOKE

# Check exit code:
# 0 = Success (first event observed)
# 1 = Exception (read logs for stack trace)
# 2 = Boot stalled (check event subscriptions)
```

**Debug with 8 Checkpoints**:
```bash
# View logs
python run_tests.py â†’ [l]

# Find where it stopped
# Look for last emoji checkpoint:
# ğŸš€ âœ… âœ… âœ… âœ… ğŸ“¨ ğŸ” âš ï¸
#         ^^^ Stopped at checkpoint 3 (SDK connected)
```

**Read the guide**:
```bash
Read docs/testing/RUNTIME_DEBUGGING.md
# Contains complete troubleshooting flowchart
```

### Run Runtime Checks

```bash
# Smoke test (8s boot validation)
python run_tests.py â†’ [s]

# Soak test (30-60s stability)
python run_tests.py â†’ [r]

# Trace mode (deep async debug)
python run_tests.py â†’ [t]

# View logs
python run_tests.py â†’ [l]

# Config snapshot
python run_tests.py â†’ [e]

# Gate: Tests + Smoke combo
python run_tests.py â†’ [g]
```

### Run Tests Manually
```bash
# Run pytest directly (colors preserved)
pytest

# With coverage
pytest --cov

# Specific marker
pytest -m unit
pytest -m integration
pytest -m runtime

# Collect only (see available tests)
pytest --collect-only
```

### Test API Connection
```bash
uv run python test_connection.py
```

### Check Project Status
```bash
# Read current status document
cat docs/current/PROJECT_STATUS.md

# Check latest test results
cat test_reports/latest.txt

# See what's implemented
find src -name "*.py" -type f | wc -l

# See all available tests
pytest --collect-only
```

---

## ğŸ“Š Progress Tracking

**âš ï¸ For current progress, ALWAYS check: `docs/current/PROJECT_STATUS.md`**

That file contains:
- Up-to-date completion percentages
- What's implemented vs what's missing
- File-by-file inventory
- Next priorities

**âš ï¸ For latest test results, ALWAYS check: `test_reports/latest.txt`**

That file contains:
- Most recent pytest run
- Pass/fail status
- Exit code
- Complete failure tracebacks

**Don't rely on cached progress data - it goes stale immediately.**

---

## ğŸš€ When User Says "Where Did We Leave Off?"

### Response Template:

```markdown
**Last Session**: 2025-10-28

**ğŸ¯ CRITICAL: Read the handoff document first!**
â†’ `AI_SESSION_HANDOFF_2025-10-28.md`

**Reading Status**:
1. âœ… Read `AI_SESSION_HANDOFF_2025-10-28.md` (latest session details)
2. âœ… Read `docs/current/PROJECT_STATUS.md` to see current progress
3. âœ… Read `test_reports/latest.txt` to see latest test results

**Status**: [Check PROJECT_STATUS.md for completion percentage]

**Latest Test Results**: [Check test_reports/latest.txt]
- All tests passing: âœ…/âŒ
- Exit code: X
- Failures: X

**What's Working** (from PROJECT_STATUS.md):
- [List from PROJECT_STATUS.md]

**What's Next** (from PROJECT_STATUS.md):
- [Next priorities from PROJECT_STATUS.md]

**Testing System**:
- âœ… Interactive test runner menu (`python run_tests.py`)
- âœ… Auto-save reports to `test_reports/latest.txt`
- âœ… Runtime Reliability Pack (smoke, soak, trace, logs, env)
- âœ… 8-checkpoint logging system for runtime debugging

See: `docs/current/PROJECT_STATUS.md` for complete details
```

---

## ğŸ¯ Next Implementation Task

**Check `docs/current/PROJECT_STATUS.md` for current priorities.**

The priorities change as work progresses. Always check the live document.

---

## âš ï¸ Critical Notes

### Architecture Clarification

**Original Specs** (`docs/PROJECT_DOCS/`):
- Written before Project-X SDK existed
- Describe manual API integration
- Direct WebSocket handling
- Custom state management

**Current Implementation** (V34):
- **SDK-First**: Project-X-Py handles heavy lifting
- **Risk Layer**: We add risk management on top
- **Async**: Modern async/await throughout
- **Event-Driven**: SDK events â†’ Risk rules â†’ Enforcement
- **Logged**: 8 strategic checkpoints for runtime debugging â­ NEW

### Key Decisions
1. **Don't reimplement what SDK provides. Use SDK, add risk logic.**
2. **Write tests first (TDD). Run pytest before runtime checks.**
3. **Check test reports FIRST before asking "what's wrong?"**
4. **Use runtime smoke test to validate deployment readiness**

---

## ğŸ§ª Test-Driven Development Workflow

**We use TDD** - Tests written first, then implementation.

**Workflow**:
1. Write test first (it fails - RED)
2. Write minimal code to pass (GREEN)
3. Refactor (REFACTOR)
4. Run `[s]` smoke test (validate runtime works)
5. Repeat

**Test Structure**: See `tests/` directory
- Use `pytest --collect-only` to see all available tests
- Use `ls -R tests/` to see current test structure
- Check `test_reports/latest.txt` for most recent results

**Complete guides**:
- `docs/testing/TESTING_GUIDE.md` - Core TDD guide
- `docs/testing/RUNTIME_DEBUGGING.md` - Runtime validation
- `docs/testing/WORKING_WITH_AI.md` - AI workflow

---

## ğŸ¤– Agent Guidelines

### For AI Agents Working on This Project

**Before Starting Work**:
1. âœ… Read `CLAUDE.md` (this file)
2. âœ… Read `docs/current/PROJECT_STATUS.md` (current state)
3. âœ… Read `test_reports/latest.txt` (latest test results)
4. âœ… Read `docs/testing/TESTING_GUIDE.md` (testing approach)
5. âœ… Read `.claude/prompts/runtime-guidelines.md` (runtime protocols)

**During Work**:
1. âœ… Write tests first (TDD)
2. âœ… Run tests via menu: `python run_tests.py`
3. âœ… Check `test_reports/latest.txt` for results
4. âœ… Use SDK where possible (don't reinvent)
5. âœ… Add SDK logging at strategic checkpoints
6. âœ… Document as you build

**Before Finishing**:
1. âœ… Run `[g]` Gate test (tests + smoke combo)
2. âœ… Check exit code 0 for smoke test
3. âœ… Update `docs/current/PROJECT_STATUS.md`
4. âœ… Verify all 8 checkpoints log correctly

**Runtime Testing Protocol**:
- After implementing feature â†’ Write tests â†’ Run tests â†’ Run smoke test
- If smoke fails â†’ Check logs â†’ Find which checkpoint failed â†’ Fix issue
- Use `docs/testing/RUNTIME_DEBUGGING.md` troubleshooting flowchart

---

## ğŸ”„ Update This File When

- [ ] Major architecture changes
- [ ] Documentation reorganization
- [ ] New critical features added (like Runtime Reliability Pack)
- [ ] Testing system changes
- [ ] Progress milestones (25% â†’ 50% â†’ etc.)
- [ ] Next priority changes

---

## âœ… Session Checklist

### At Start of Session
- [ ] Read `CLAUDE.md` (this file)
- [ ] Read `docs/current/PROJECT_STATUS.md` (current state)
- [ ] Read `test_reports/latest.txt` (latest test results)
- [ ] Read `docs/testing/README.md` (testing navigation)
- [ ] Check what tests exist (`pytest --collect-only`)

### During Session
- [ ] Write tests first (TDD)
- [ ] Use SDK where possible
- [ ] Add SDK logging at checkpoints if touching core files
- [ ] Run tests via menu (`python run_tests.py`)
- [ ] Check `test_reports/latest.txt` after each run
- [ ] Document as you build

### Before Deploying
- [ ] Run `[g]` Gate test (full suite + smoke)
- [ ] Verify exit code 0
- [ ] Check all 8 checkpoints log correctly
- [ ] Run `[r]` Soak test for major changes

### End of Session
- [ ] Run full test suite (`python run_tests.py` â†’ `[1]`)
- [ ] Run smoke test (`[s]`)
- [ ] Update `docs/current/PROJECT_STATUS.md` if progress made
- [ ] Archive old docs if major changes
- [ ] Git commit with clear message

---

## ğŸ“ Understanding the Project in 5 Minutes

**What**: Trading risk manager for TopstepX futures accounts
**Why**: Prevent account violations, automate risk enforcement
**How**: SDK handles trading, we add risk rules on top

**12 Risk Rules**: See `docs/PROJECT_DOCS/rules/` for specifications
- Check `src/risk_manager/rules/` to see which are implemented
- Check `docs/current/PROJECT_STATUS.md` for completion status

**Tech Stack**:
- Python 3.12+ async
- Project-X-Py SDK v3.5.9 (handles all trading)
- Pydantic (config/validation)
- SQLite (state persistence)
- pytest (TDD testing)
- Runtime Reliability Pack (runtime validation) â­ NEW

**Testing System**:
- Interactive menu: `python run_tests.py`
- Auto-save reports: `test_reports/latest.txt`
- Runtime checks: Smoke, Soak, Trace, Logs, Env
- 8-checkpoint logging: Find exactly where runtime fails

---

## ğŸ’¡ If You're Confused

**Read in this exact order**:
1. This file (`CLAUDE.md`)
2. `test_reports/latest.txt` - See if tests are passing
3. `docs/current/PROJECT_STATUS.md` - See what's done
4. `docs/testing/README.md` - Understand testing system
5. `docs/current/SDK_INTEGRATION_GUIDE.md` - Understand SDK-first approach

**Still confused?**
- Look at `examples/` directory - See it working
- Run `python run_tests.py â†’ [s]` - Runtime smoke test
- Read `docs/testing/RUNTIME_DEBUGGING.md` - Complete troubleshooting guide
- Read `docs/PROJECT_DOCS/INTEGRATION_NOTE.md` - Understand spec history

---

## ğŸš€ Ready to Code?

**Quick Start**:
```bash
# 1. Check latest test results
cat test_reports/latest.txt

# 2. Read current status
cat docs/current/PROJECT_STATUS.md

# 3. Pick a task from PROJECT_STATUS.md

# 4. Write test first
# Create test in tests/unit/ or tests/integration/

# 5. Run tests
python run_tests.py â†’ [2] for unit tests

# 6. Check results
cat test_reports/latest.txt

# 7. Implement feature
# Create code in src/

# 8. Run tests again
python run_tests.py â†’ [2]

# 9. Run smoke test (validates runtime works)
python run_tests.py â†’ [s]
# Check exit code: 0 = good, 1 = exception, 2 = stalled

# 10. Update PROJECT_STATUS.md when done
```

**Let's build! ğŸ›¡ï¸**

---

**Last Updated**: 2025-10-23
**Next Update**: When testing system or documentation changes significantly
**Maintainer**: Update this when project structure or testing system changes

---

## ğŸ“‹ Quick Reference Card

**Most Important Files for Agents**:
1. `CLAUDE.md` â† You are here
2. `docs/current/PROJECT_STATUS.md` â† Current progress
3. `test_reports/latest.txt` â† Latest test results
4. `docs/testing/TESTING_GUIDE.md` â† How to test
5. `docs/testing/RUNTIME_DEBUGGING.md` â† How to debug runtime

**Most Important Commands**:
1. `python run_tests.py` â† Run tests with menu
2. `cat test_reports/latest.txt` â† Check latest results
3. `cat docs/current/PROJECT_STATUS.md` â† Check progress
4. `pytest --collect-only` â† See available tests
5. `python run_tests.py â†’ [s]` â† Runtime smoke test

**Exit Codes to Know**:
- `0` = Success
- `1` = Exception (check logs)
- `2` = Boot stalled (check event subscriptions)

**8 Checkpoints to Look For**:
```
ğŸš€ Service Start
âœ… Config Loaded
âœ… SDK Connected
âœ… Rules Initialized
âœ… Event Loop Running
ğŸ“¨ Event Received
ğŸ” Rule Evaluated
âš ï¸ Enforcement Triggered
```

**That's everything. Welcome to Risk Manager V34! ğŸ¯**
